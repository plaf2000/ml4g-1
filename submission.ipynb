{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import pyBigWig\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNALS_CNN = ['DNase', 'H3K4me1', 'H3K4me3', 'H3K27ac', 'H3K36me3']   # List of signal tracks to be used\n",
    "N_SIGNALS_CNN = len(SIGNALS_CNN)\n",
    "SIGNAL_CNN_WINDOW = 1e4 # Window size in base pairs\n",
    "N_BINS = 100 # Number of base pairs per bin\n",
    "PREPROCESSED_BASE_PATH = \"Data/preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "\n",
    "half_window = int(SIGNAL_CNN_WINDOW // 2)\n",
    "chromstrs = []\n",
    "ranges = []\n",
    "\n",
    "\n",
    "def get_signals_bins(df: pd.DataFrame, cell_type: str, n_bins: int=N_BINS) -> np.ndarray:\n",
    "    df[\"neg_strand\"] = mask = df['strand'] == '-'\n",
    "    df.loc[mask, ['TSS_start', 'TSS_end']] = df.loc[mask, ['TSS_end', 'TSS_start']].values\n",
    "    df[\"center\"] = ((df[\"TSS_start\"] + df[\"TSS_end\"]) // 2).astype(int)\n",
    "    df[\"window_start\"] = df[\"center\"] - half_window\n",
    "    df[\"window_end\"] = df[\"center\"] + half_window\n",
    "\n",
    "    bins_signal_gene = np.zeros((len(df), len(SIGNALS_CNN), N_BINS))\n",
    "    for i, signal in enumerate(SIGNALS_CNN):\n",
    "        print(\"Processing signal:\", signal, f\"{i+1}/{len(SIGNALS_CNN)}\")\n",
    "        fname = glob.glob(f\"Data/bigwig/{signal}-bigwig/{cell_type}*\")[0]\n",
    "        bw = pyBigWig.open(fname)\n",
    "        for j, (chromstr, window_start, window_end, neg_strand) in enumerate(tqdm(df[[\"chr\", \"window_start\", \"window_end\", \"neg_strand\"]].itertuples(index=False), total=len(df))):\n",
    "            bins = bw.stats(chromstr, window_start, window_end, type=\"mean\", nBins=n_bins)\n",
    "            if neg_strand:\n",
    "                bins = bins[::-1]\n",
    "            bins_signal_gene[j, i] = bins\n",
    "\n",
    "        bw.close()\n",
    "            \n",
    "    return bins_signal_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train set.\n",
      "Processing cell type: X1\n",
      "Processing signal: DNase 1/5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing cell type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./Data/CAGE-train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_test_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_info.tsv\u001b[39m\u001b[33m'\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m, usecols=[\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m4\u001b[39m,\u001b[32m5\u001b[39m,\u001b[32m6\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m signal_bins = \u001b[43mget_signals_bins\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_BINS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m np.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREPROCESSED_BASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/cnn_input_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_test_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_BINS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m, signal_bins)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_signals_bins\u001b[39m\u001b[34m(df, cell_type, n_bins)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(SIGNALS_CNN):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing signal:\u001b[39m\u001b[33m\"\u001b[39m, signal, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(SIGNALS_CNN)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     fname = \u001b[43mglob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mData/bigwig/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msignal\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-bigwig/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcell_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m     bw = pyBigWig.open(fname)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j, (chromstr, window_start, window_end, neg_strand) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(df[[\u001b[33m\"\u001b[39m\u001b[33mchr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwindow_start\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwindow_end\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mneg_strand\u001b[39m\u001b[33m\"\u001b[39m]].itertuples(index=\u001b[38;5;28;01mFalse\u001b[39;00m), total=\u001b[38;5;28mlen\u001b[39m(df))):\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "cell_types_per_set = {\n",
    "    \"train\": [\"X1\", \"X2\"],\n",
    "    \"val\": [\"X1\", \"X2\"],\n",
    "    \"test\": [\"X3\"]\n",
    "}\n",
    "\n",
    "for train_test_val, cell_types in cell_types_per_set.items():\n",
    "    print(f\"Processing {train_test_val} set.\")\n",
    "    for cell_type in cell_types:\n",
    "        print(f\"Processing cell type: {cell_type}\")\n",
    "        df = pd.read_csv(f'./Data/CAGE-train/{cell_type}_{train_test_val}_info.tsv', sep='\\t', usecols=[0,1,4,5,6])\n",
    "        signal_bins = get_signals_bins(df, cell_type, n_bins=N_BINS)\n",
    "        np.save(f'{PREPROCESSED_BASE_PATH}/cnn_input_{cell_type}_{train_test_val}_{N_BINS}.npy', signal_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/preprocessed/cnn_input_X1_train_100.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m all_data = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cell_type \u001b[38;5;129;01min\u001b[39;00m cell_types:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPREPROCESSED_BASE_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cnn_input_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcell_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_test_val\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mN_BINS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.npy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     all_data.append(data)\n\u001b[32m      6\u001b[39m grouped = np.concatenate(all_data, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edonn\\miniconda3\\envs\\ml4g\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data/preprocessed/cnn_input_X1_train_100.npy'"
     ]
    }
   ],
   "source": [
    "for train_test_val, cell_types in cell_types_per_set.items():\n",
    "    all_data = []\n",
    "    for cell_type in cell_types:\n",
    "        data = np.load(f'{PREPROCESSED_BASE_PATH}/cnn_input_{cell_type}_{train_test_val}_{N_BINS}.npy')\n",
    "        all_data.append(data)\n",
    "    grouped = np.concatenate(all_data, axis=0)\n",
    "    np.save(f\"{PREPROCESSED_BASE_PATH}/cnn_input_{train_test_val}.npy\", grouped)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/preprocessed/cnn_y_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     y_df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData/CAGE-train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_test_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_y.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     all_y.append(y_df[\u001b[33m\"\u001b[39m\u001b[33mgex\u001b[39m\u001b[33m\"\u001b[39m].to_numpy())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPREPROCESSED_BASE_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cnn_y_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_test_val\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m       \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edonn\\miniconda3\\envs\\ml4g\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:580\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(file, arr, allow_pickle, fix_imports)\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    579\u001b[39m         file = file + \u001b[33m'\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     file_ctx = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[32m    583\u001b[39m     arr = np.asanyarray(arr)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data/preprocessed/cnn_y_train.npy'"
     ]
    }
   ],
   "source": [
    "for train_test_val, cell_types in cell_types_per_set.items():\n",
    "    if train_test_val == \"test\":\n",
    "        continue\n",
    "    all_y = []\n",
    "    for cell_type in cell_types:\n",
    "        y_df = pd.read_csv(f\"Data/CAGE-train/{cell_type}_{train_test_val}_y.tsv\", sep=\"\\t\")\n",
    "        all_y.append(y_df[\"gex\"].to_numpy())\n",
    "    np.save(f\"{PREPROCESSED_BASE_PATH}/cnn_y_{train_test_val}.npy\", np.concatenate(all_y, axis=0))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used to estimate the gene expression from binned signal is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(N_SIGNALS_CNN, 32, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(32, 16, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(16, 1, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LazyLinear(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, features=2, genes)\n",
    "        out = self.net(x)\n",
    "        return out.squeeze(1)  # shape: (batch, genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Since we are scoring with Spearman correlation, by which only the rank of the predicted elements is considered, we train our model to predict the $\\log(y + 1)$. This doesn't affect the score, since $\\log$ is monotonically increasing, and will help the training, given that some very high values in $y$ are difficult to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigWigDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.missing_val = np.any(np.isnan(X) | np.isinf(X), axis=(1,2))\n",
    "        X = X[~self.missing_val, :, :]\n",
    "        y = y[~self.missing_val]\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_orig = torch.from_numpy(y).float()\n",
    "        self.y = torch.log1p(self.y_orig)  # log(y + 1) transformation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26b24d3eb50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2056\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "SAVE_BASED_ON_SPEARMAN = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/preprocessed/cnn_input_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x_train = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPREPROCESSED_BASE_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cnn_input_train.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m y_train = np.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREPROCESSED_BASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/cnn_y_train.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m x_val = np.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREPROCESSED_BASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/cnn_input_val.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edonn\\miniconda3\\envs\\ml4g\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data/preprocessed/cnn_input_train.npy'"
     ]
    }
   ],
   "source": [
    "x_train = np.load(f\"{PREPROCESSED_BASE_PATH}/cnn_input_train.npy\")\n",
    "y_train = np.load(f\"{PREPROCESSED_BASE_PATH}/cnn_y_train.npy\")\n",
    "\n",
    "x_val = np.load(f\"{PREPROCESSED_BASE_PATH}/cnn_input_val.npy\")\n",
    "y_val = np.load(f\"{PREPROCESSED_BASE_PATH}/cnn_y_val.npy\")\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    BigWigDataset(x_train, y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "x_val_torch, y_val_torch = BigWigDataset(x_val, y_val)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m running_loss = \u001b[32m.0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mtraining_loader\u001b[49m), total=\u001b[38;5;28mlen\u001b[39m(training_loader)):            \n\u001b[32m     20\u001b[39m     optimizer.zero_grad()\n\u001b[32m     21\u001b[39m     pred = net(X)\n",
      "\u001b[31mNameError\u001b[39m: name 'training_loader' is not defined"
     ]
    }
   ],
   "source": [
    "net = GeneCNNModel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "best_val_loss = float('inf')\n",
    "best_spearman_corr = float('-inf')\n",
    "spearman_model_path = None\n",
    "val_model_path = None\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "for epoch in range(EPOCHS):\n",
    "    best_loss = float(\"inf\")\n",
    "    last_loss = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    running_loss = .0\n",
    "    print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, (X, y) in tqdm(enumerate(training_loader), total=len(training_loader)):            \n",
    "        optimizer.zero_grad()\n",
    "        pred = net(X)\n",
    "        loss = loss_fn(pred.flatten(), y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        running_loss += batch_loss\n",
    "        best_loss = min(batch_loss, best_loss)\n",
    "    \n",
    "    \n",
    "\n",
    "    avg_loss = running_loss / (i + 1)\n",
    "\n",
    "    print(f\"Training loss: best {best_loss} and avg {avg_loss}\")\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    running_vloss = .0\n",
    "\n",
    "    pred_val = net(x_val_torch)\n",
    "    loss = loss_fn(pred_val.flatten(), y_val_torch)\n",
    "    val_loss = loss.item()\n",
    "    \n",
    "    print('Validation loss: {}'.format(val_loss))\n",
    "\n",
    "    spearman_corr = spearmanr(pred_val.detach().numpy(), y_val_torch.detach().numpy()).statistic\n",
    "    print(\"Spearman's correlation:\", spearman_corr)\n",
    "    \n",
    "    def save_model(criterion: str, model_path: str | None) -> str:\n",
    "        if model_path is not None and os.path.exists(model_path):\n",
    "            os.remove(model_path)\n",
    "\n",
    "        model_path = \"models/cnn_{}_{}_BS{}_LR{}_BIN{}_W{}_EPOCH{}\".format(timestamp, criterion, BATCH_SIZE, LEARNING_RATE, int(N_BINS), int(SIGNAL_CNN_WINDOW), epoch + 1)\n",
    "\n",
    "        print(f\"Saving as best model in {model_path}\")\n",
    "        torch.save(net, model_path)\n",
    "\n",
    "        return model_path\n",
    "    \n",
    "    \n",
    "    if spearman_corr > best_spearman_corr:\n",
    "        best_spearman_corr = spearman_corr\n",
    "        if SAVE_BASED_ON_SPEARMAN:\n",
    "            spearman_model_path = save_model(\"SPEARMAN{:.4f}\".format(best_spearman_corr * 100), spearman_model_path)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        val_model_path = save_model(\"VALLOSS{:.4f}\".format(best_val_loss), val_model_path)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "print(\"Training complete. Best avg validation loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = np.load(f\"{PREPROCESSED_BASE_PATH}/cnn_input_test.npy\")\n",
    "\n",
    "if SAVE_BASED_ON_SPEARMAN:\n",
    "    cnn_net = torch.load(spearman_model_path, weights_only=False) # Load the best model based on Spearman correlation\n",
    "else:\n",
    "    cnn_net = torch.load(val_model_path, weights_only=False) # Load the best model based on validation loss\n",
    "\n",
    "# Or manually specify the model path:\n",
    "# cnn_net = torch.load(\"models/cnn_your_model_path_here\", weights_only=False)\n",
    "\n",
    "cnn_net.eval()\n",
    "y_test_pred = cnn_net(torch.tensor(x_test, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "test_genes = pd.read_csv('./Data/CAGE-train/X3_test_info.tsv', sep='\\t', usecols=[0,1])\n",
    "test_genes['gex_predicted'] = np.exp(y_test_pred.flatten()) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'Data/submission'\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Laffranchi_Paolo_Project1.zip\"\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
